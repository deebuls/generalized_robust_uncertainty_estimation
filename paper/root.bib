% Encoding: UTF-8
@inproceedings{nair_laplace_2O22,
  author       = {Deebul S. Nair and
                  Nico Hochgeschwender and
                  Miguel A. Olivares{-}M{\'{e}}ndez},
  editor       = {Gabriel Pedroza and
                  Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo and
                  Xin Cynthia Chen and
                  Xiaowei Huang and
                  Hu{\'{a}}scar Espinoza and
                  Mauricio Castillo{-}Effen and
                  John A. McDermid and
                  Richard Mallah and
                  Se{\'{a}}n {\'{O}} h{\'{E}}igeartaigh},
  title        = {Maximum Likelihood Uncertainty Estimation: Robustness to Outliers},
  booktitle    = {Proceedings of the Workshop on Artificial Intelligence Safety 2022
                  (SafeAI 2022) co-located with the Thirty-Sixth {AAAI} Conference on
                  Artificial Intelligence (AAAI2022), Virtual, February, 2022},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {3087},
  publisher    = {CEUR-WS.org},
  year         = {2022},
  url          = {https://ceur-ws.org/Vol-3087/paper\_20.pdf},
  timestamp    = {Fri, 10 Mar 2023 16:22:15 +0100},
}

@incollection{hutchison_indoor_2012,
	address = {Berlin, Heidelberg},
	title = {Indoor {Segmentation} and {Support} {Inference} from {RGBD} {Images}},
	volume = {7576},
	isbn = {978-3-642-33714-7 978-3-642-33715-4},
	abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into ﬂoor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We oﬀer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.},
	language = {en},
	urldate = {2021-05-17},
	booktitle = {Computer {Vision} – {ECCV} 2012},
	author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
	year = {2012},
	doi = {10.1007/978-3-642-33715-4_54},
	pages = {746--760},
	
}
@article{gneiting2007strictly,
	title={Strictly proper scoring rules, prediction, and estimation},
	author={Gneiting, Tilmann and Raftery, Adrian E},
	journal={Journal of the American statistical Association},
	volume={102},
	number={477},
	pages={359--378},
	year={2007},
	publisher={Taylor \& Francis}
}



@inproceedings{barron2019general,
	title={A general and adaptive robust loss function},
	author={Barron, Jonathan T},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={4331--4339},
	year={2019}
}
@inproceedings{lathuiliere2018deepgum,
	title={Deepgum: Learning deep robust regression with a gaussian-uniform mixture model},
	author={Lathuili{\`e}re, St{\'e}phane and Mesejo, Pablo and Alameda-Pineda, Xavier and Horaud, Radu},
	booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
	pages={202--217},
	year={2018}
}





@INPROCEEDINGS{huang_2018,
	author={Huang, Xinyu and Cheng, Xinjing and Geng, Qichuan and Cao, Binbin and Zhou, Dingfu and Wang, Peng and Lin, Yuanqing and Yang, Ruigang},
	booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}, 
	title={The ApolloScape Dataset for Autonomous Driving}, 
	year={2018},
	pages={1067-10676},
	doi={10.1109/CVPRW.2018.00141}}


@inproceedings{goodfellow_2015,
	title	= {Explaining and Harnessing Adversarial Examples},
	author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
	year	= {2015},

	booktitle	= {International Conference on Learning Representations}
}



@article{handa_etal_2014,
	title   = {A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM},
	author  = {Handa, Ankur and Whelan, Thomas and McDonald, John and Davison, Andrew J},
	journal = {ICRA},
	year    = {2014},
}

@article{Barron_eta_2013A,
	author  = {Jonathan T. Barron and Jitendra Malik},
	title   = {Intrinsic Scene Properties from a Single RGB-D Image},
	journal = {CVPR},
	year    = {2013},
}

@article{Bohg_etal_2014,
	title   = {Robot arm pose estimation through pixel-wise part classification},
	author  = {Bohg, Jeannette and Romero, Javier and Herzog, Alexander and Schaal, Stefan},
	journal = {ICRA},
	year    = {2014},
}

@inproceedings{ronneberger2015u,
	title={U-net: Convolutional networks for biomedical image segmentation},
	author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle={International Conference on Medical image computing and computer-assisted intervention},
	pages={234--241},
	year={2015},
	organization={Springer}
}



@book{huber2004robust,
	title={Robust statistics},
	author={Huber, Peter J},
	volume={523},
	year={2004},
	publisher={John Wiley \& Sons}
}


@article{mcleish1982robust,
	title={A robust alternative to the normal distribution},
	author={McLeish, Donald Leslie},
	journal={The Canadian Journal of Statistics/La Revue Canadienne de Statistique},
	pages={89--102},
	year={1982},
	publisher={JSTOR}
}


@book{bosse2016robust,
	title={Robust estimation and applications in robotics},
	author={Bosse, Michael and Agamennoni, Gabriel and Gilitschenski, Igor and others},
	year={2016},
	publisher={Now Publishers}
}


@inproceedings{nix1994estimating,
	title={Estimating the mean and variance of the target probability distribution},
	author={Nix, David A and Weigend, Andreas S},
	booktitle={Proceedings of 1994 ieee international conference on neural networks (ICNN'94)},
	volume={1},
	pages={55--60},
	year={1994},
	organization={IEEE}
}

@inproceedings{blundell2015weight,
	title={Weight uncertainty in neural network},
	author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	booktitle={International Conference on Machine Learning},
	pages={1613--1622},
	year={2015},
	organization={PMLR}
}


@article{sun_reliability_2020,
	title = {Reliability {Validation} of {Learning} {Enabled} {Vehicle} {Tracking}},
	abstract = {This paper studies the reliability of a real-world learning-enabled system, which conducts dynamic vehicle tracking based on a high-resolution wide-area motion imagery input. The system consists of multiple neural network components –to process the imagery inputs – and multiple symbolic (Kalman ﬁlter) components – to analyse the processed information for vehicle tracking. It is known that neural networks suffer from adversarial examples, which make them lack robustness. However, it is unclear if and how the adversarial examples over learning components can affect the overall system-level reliability. By integrating a coverage-guided neural network testing tool, DeepConcolic, with the vehicle tracking system, we found that (1) the overall system can be resilient to some adversarial examples thanks to the existence of other components, and (2) the overall system presents an extra level of uncertainty which cannot be determined by analysing the deep learning components only. This research suggests the need for novel veriﬁcation and validation methods for learning-enabled systems.},
	language = {en},
	urldate = {2020-07-16},
	journal = {arXiv:2002.02424 [cs]},
	author = {Sun, Youcheng and Zhou, Yifan and Maskell, Simon and Sharp, James and Huang, Xiaowei},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.02424},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Important, systems},
	
}
@inproceedings{dusenberry2020analyzing,
	title={Analyzing the role of model uncertainty for electronic health records},
	author={Dusenberry, Michael W and Tran, Dustin and Choi, Edward and Kemp, Jonas and Nixon, Jeremy and Jerfel, Ghassen and Heller, Katherine and Dai, Andrew M},
	booktitle={Proceedings of the ACM Conference on Health, Inference, and Learning},
	pages={204--213},
	year={2020}
}


@ARTICLE{loquercio_general_2020,

author={Loquercio, Antonio and Segu, Mattia and Scaramuzza, Davide},

journal={IEEE Robotics and Automation Letters}, 

title={A General Framework for Uncertainty Estimation in Deep Learning}, 

year={2020},

volume={5},

number={2},

pages={3153-3160},

doi={10.1109/LRA.2020.2974682}}


@article{sensoy_evidential_2018,
 author    = {Murat Sensoy and
Lance M. Kaplan and
Melih Kandemir},
editor    = {Samy Bengio and
Hanna M. Wallach and
Hugo Larochelle and
Kristen Grauman and
Nicol{\`{o}} Cesa{-}Bianchi and
Roman Garnett},
title     = {Evidential Deep Learning to Quantify Classification Uncertainty},
booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montr{\'{e}}al, Canada},
pages     = {3183--3193},
year      = {2018},
}

@inproceedings{amini_deep_2019,
	title = {Deep {Evidential} {Regression}},
	volume = {33},
  	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Amini, Alexander and Schwarting, Wilko and Soleimany, Ava and Rus, Daniela},

	year = {2020},
	pages = {14927--14937}
}

@inproceedings{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	volume = {30},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},

	author = {Kendall, Alex and Gal, Yarin},

	year = {2017}
}
@article{malinin_predictive_nodate,
	title = {Predictive {Uncertainty} {Estimation} via {Prior} {Networks}},
	abstract = {Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been deﬁned and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models distributional uncertainty. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classiﬁcation and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassiﬁcation on the MNIST and CIFAR-10 datasets, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.},
	language = {en},
	author = {Malinin, Andrey and Gales, Mark},
	keywords = {Important},
	pages = {12},
	file = {Malinin and Gales - Predictive Uncertainty Estimation via Prior Networ.pdf:/home/deebuls/Zotero/storage/HUPXKSTG/Malinin and Gales - Predictive Uncertainty Estimation via Prior Networ.pdf:application/pdf}
}

@article{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}:  {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classiﬁcation do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacriﬁcing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classiﬁcation, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and ﬁnish by using dropout’s uncertainty in deep reinforcement learning.},
	language = {en},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	year = {2016},
	pages = {10},
	file = {Gal and Ghahramani - Dropout as a Bayesian Approximation  Representing.pdf:/home/deebuls/Zotero/storage/LK8GN3F6/Gal and Ghahramani - Dropout as a Bayesian Approximation  Representing.pdf:application/pdf}
}
@inproceedings{lakshminarayanan_simple_2016,
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	booktitle = {Advances in Neural Information Processing Systems},

	pages = {},

	title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},

	volume = {30},
	year = {2017}
}
@article{AlganU21,
	author    = {G{\"{o}}rkem Algan and
	Ilkay Ulusoy},
	title     = {Image classification with deep learning in the presence of noisy labels:
	{A} survey},
	journal   = {Knowl. Based Syst.},
	volume    = {215},
	pages     = {106771},
	year      = {2021},
}
@inproceedings{dusenberry2020efficient,
	title={Efficient and scalable bayesian neural nets with rank-1 factors},
	author={Dusenberry, Michael and Jerfel, Ghassen and Wen, Yeming and Ma, Yian and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
	booktitle={International conference on machine learning},
	pages={2782--2792},
	year={2020},
	organization={PMLR}
}

@inproceedings{wen2020batchensemble,
	title={Batchensemble: an alternative approach to efficient ensemble and lifelong learning},
	author={Wen, Yeming and Tran, Dustin and Ba, Jimmy},
	booktitle={ International Conference on Learning Representations},
	year={2020}
}

@inproceedings{malinin2018predictive,
	title={Predictive uncertainty estimation via prior networks},
	author={Malinin, Andrey and Gales, Mark},
	booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	pages={7047--7058},
	year={2018}
}


@inproceedings{calandra2016manifold,
	title={Manifold Gaussian processes for regression},
	author={Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
	booktitle={2016 International Joint Conference on Neural Networks (IJCNN)},
	pages={3338--3345},
	year={2016},
	organization={IEEE}
}

@article{tagasovska2019single,
	title={Single-Model Uncertainties for Deep Learning},
	author={Tagasovska, Natasa and Lopez-Paz, David},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	pages={6417--6428},
	year={2019}
}



@inproceedings{Riquelme2018Deep,
	author    = {Carlos Riquelme and
	George Tucker and
	Jasper Snoek},
	title     = {Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian
	Deep Networks for Thompson Sampling},
	booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
	Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},

	year      = {2018},
}



@inproceedings{SnoekRSKSSPPA15,
	author    = {Jasper Snoek and
	Oren Rippel and
	Kevin Swersky and
	Ryan Kiros and
	Nadathur Satish and
	Narayanan Sundaram and
	Md. Mostofa Ali Patwary and
	Prabhat and
	Ryan P. Adams},
	title     = {Scalable Bayesian Optimization Using Deep Neural Networks},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
	{ICML} 2015, Lille, France, 6-11 July 2015},
	series    = {{JMLR} Workshop and Conference Proceedings},
	volume    = {37},
	pages     = {2171--2180},
	publisher = {JMLR.org},
	year      = {2015},
}



@inproceedings{LiuLPTBL20,
	author    = {Jeremiah Z. Liu and
	Zi Lin and
	Shreyas Padhy and
	Dustin Tran and
	Tania Bedrax{-}Weiss and
	Balaji Lakshminarayanan},
	title     = {Simple and Principled Uncertainty Estimation with Deterministic Deep
	Learning via Distance Awareness},
	booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
	on Neural Information Processing Systems 2020, NeurIPS 2020, December
	6-12, 2020, virtual},
	year      = {2020},
}

@inproceedings{van2020uncertainty,
	title={Uncertainty estimation using a single deep deterministic neural network},
	author={Van Amersfoort, Joost and Smith, Lewis and Teh, Yee Whye and Gal, Yarin},
	booktitle={International Conference on Machine Learning},
	pages={9690--9700},
	year={2020},
	organization={PMLR}
}



@article{malinin_regression_2020,
	title = {Regression {Prior} {Networks}},
	abstract = {Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via Ensemble Distribution Distillation (EnD\${\textasciicircum}2\$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD\${\textasciicircum}2\$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.},
	urldate = {2020-12-23},
	journal = {arXiv:2006.11590 [cs, stat]},
	author = {Malinin, Andrey and Chervontsev, Sergey and Provilkov, Ivan and Gales, Mark},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{havasi_training_2021,
	title = {{TRAINING} {INDEPENDENT} {SUBNETWORKS} {FOR} {ROBUST} {PREDICTION}},
	abstract = {Recent approaches to efﬁciently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a signiﬁcant computational cost. In this work, we show a surprising result: the beneﬁts of using multiple predictions can be achieved ‘for free’ under a single model’s forward pass. In particular, we show that, using a multi-input multi-output (MIMO) conﬁguration, one can utilize a single model’s capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a signiﬁcant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.},
	language = {en},
	author = {Havasi, Marton and Jenatton, Rodolphe and Fort, Stanislav},
	year = {2021},
	pages = {13},
	file = {Havasi et al. - 2021 - TRAINING INDEPENDENT SUBNETWORKS FOR ROBUST PREDIC.pdf:/home/deebuls/Zotero/storage/XH6VDBM7/Havasi et al. - 2021 - TRAINING INDEPENDENT SUBNETWORKS FOR ROBUST PREDIC.pdf:application/pdf}
}

@article{jha_safe_2018,
	title = {Safe {Autonomy} {Under} {Perception} {Uncertainty} {Using} {Chance}-{Constrained} {Temporal} {Logic}},
	volume = {60},
	issn = {0168-7433, 1573-0670},
	doi = {10.1007/s10817-017-9413-9},
	abstract = {Autonomous vehicles have found wide-ranging adoption in aerospace, terrestrial as well as marine use. These systems often operate in uncertain environments and in the presence of noisy sensors, and use machine learning and statistical sensor fusion algorithms to form an internal model of the world that is inherently probabilistic. Autonomous vehicles need to operate using this uncertain world-model, and hence, their correctness cannot be deterministically speciﬁed. Even once probabilistic correctness is speciﬁed, proving that an autonomous vehicle will operate correctly is a challenging problem. In this paper, we address these challenges by proposing a correct-by-synthesis approach to autonomous vehicle control. We propose a probabilistic extension of temporal logic, named Chance Constrained Temporal Logic (C2TL), that can be used to specify correctness requirements in presence of uncertainty. C2TL extends temporal logic by including chance constraints as predicates in the formula which allows modeling of perception uncertainty while retaining its ease of reasoning. We present a novel automated synthesis technique that compiles C2TL speciﬁcation into mixed integer constraints, and uses second-order (quadratic) cone programming to synthesize optimal control of autonomous vehicles subject to the C2TL speciﬁcation. We also present a risk distribution approach that enables synthesis of plans with lower cost without increasing the overall risk. We demonstrate the effectiveness of the proposed approach on a diverse set of illustrative examples.},
	language = {en},
	number = {1},
	urldate = {2020-04-06},
	journal = {J Autom Reasoning},
	author = {Jha, Susmit and Raman, Vasumathi and Sadigh, Dorsa and Seshia, Sanjit A.},
	month = jan,
	year = {2018},
	keywords = {Important},
	pages = {43--62},
	
}

@article{serban_towards_2020,
	title = {Towards {Using} {Probabilistic} {Models} to {Design} {Software} {Systems} with {Inherent} {Uncertainty}},
	abstract = {The adoption of machine learning (ML) components in software systems raises new engineering challenges. In particular, the inherent uncertainty regarding functional suitability and the operation environment makes architecture evaluation and trade-oﬀ analysis diﬃcult. We propose a software architecture evaluation method called Modeling Uncertainty During Design (MUDD) that explicitly models the uncertainty associated to ML components and evaluates how it propagates through a system. The method supports reasoning over how architectural patterns can mitigate uncertainty and enables comparison of diﬀerent architectures focused on the interplay between ML and classical software components. While our approach is domain-agnostic and suitable for any system where uncertainty plays a central role, we demonstrate our approach using as example a perception system for autonomous driving.},
	language = {en},
	urldate = {2020-09-02},
	journal = {arXiv:2008.03046 [cs]},
	author = {Serban, Alex and Poll, Erik and Visser, Joost},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Software Engineering, Important, design space},
	
}
@inproceedings{schwalbe_survey_2020,
	address = {Toulouse, France},
	title = {A {Survey} on {Methods} for the {Safety} {Assurance} of {Machine} {Learning} {Based} {Systems}},
	abstract = {Methods for safety assurance suggested by the ISO 26262 automotive functional safety standard are not sufficient for applications based on machine learning (ML). We provide a structured, certification oriented overview on available methods supporting the safety argumen-tation of a ML based system. It is sorted into life-cycle phases, and maturity of the approach as well as applicability to different ML types are collected. From this we deduce current open challenges: powerful solvers, inclusion of expert knowledge, validation of data representativity and model diversity, and model introspection with provable guarantees.},
	urldate = {2021-05-12},
	booktitle = {10th {European} {Congress} on {Embedded} {Real} {Time} {Software} and {Systems} ({ERTS} 2020)},
	author = {Schwalbe, Gesina and Schels, Martin},
	month = jan,
	year = {2020},
	keywords = {explainable AI, functional safety, ISO 26262, life-cycle, machine learning},
	
}

@article{borg_safely_2019,
	title = {Safely {Entering} the {Deep}: {A} {Review} of {Verification} and {Validation} for {Machine} {Learning} and a {Challenge} {Elicitation} in the {Automotive} {Industry}},
	volume = {1},
	issn = {2589-2258},
	shorttitle = {Safely {Entering} the {Deep}},
	doi = {10.2991/jase.d.190131.001},
	abstract = {Deep neural networks (DNNs) will emerge as a cornerstone in automotive software engineering. However, developing systems with DNNs introduces novel challenges for safety assessments. This paper reviews the state-of-the-art in verification and validation of safety-critical systems that rely on machine learning. Furthermore, we report from a workshop...},
	language = {en},
	number = {1},
	urldate = {2021-05-12},
	journal = {Journal of Automotive Software Engineering},
	author = {Borg, Markus and Englund, Cristofer and Wnuk, Krzysztof and Duran, Boris and Levandowski, Christoffer and Gao, Shenjian and Tan, Yanwen and Kaijser, Henrik and Lönn, Henrik and Törnqvist, Jonas},
	month = jan,
	year = {2019},
	pages = {1--19},
	
}

@InProceedings{Winery,
  author    = {Oliver Kopp and others},
  title     = {{Winery -- A Modeling Tool for {TOSCA}-based Cloud Applications}},
  booktitle = {Proceedings of 11\textsuperscript{th} International Conference on Service-Oriented Computing (ICSOC'13)},
  year      = {2013},
  volume    = {8274},
  series    = {LNCS},
  pages     = {700--704},
  publisher = {Springer Berlin Heidelberg},
  doi       = {10.1007/978-3-642-45005-1_64},
  keywords  = {Cloud Applications; Modeling; TOSCA; Management; Portability},
}

@Article{Binz2009,
  author      = {Tobias Binz and Gerd Breiter and Frank Leymann and Thomas Spatzier},
  title       = {{Portable Cloud Services Using TOSCA}},
  journal     = {IEEE Internet Computing},
  year        = {2012},
  pages       = {80--85},
  volume      = {16},
  number      = {03},
  issn        = {1089-7801},
  doi         = {10.1109/mic.2012.43},
  month       = may,
  publisher   = {IEEE Computer Society},
}

@Manual{mwe,
  author = {Martin Scharrer},
  title  = {The \texttt{mwe} Package},
  year   = {2017},
}

@article{west1984outlier,
	title={Outlier models and prior distributions in Bayesian linear regression},
	author={West, Mike},
	journal={Journal of the Royal Statistical Society: Series B (Methodological)},
	volume={46},
	number={3},
	pages={431--439},
	year={1984},
	publisher={Wiley Online Library}
}

@inproceedings{arazo2019unsupervised,
	title={Unsupervised label noise modeling and loss correction},
	author={Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel and McGuinness, Kevin},
	booktitle={International Conference on Machine Learning},
	pages={312--321},
	year={2019},
	organization={PMLR}
}
@article{xia2019anchor,
	title={Are anchor points really indispensable in label-noise learning?},
	author={Xia, Xiaobo and Liu, Tongliang and Wang, Nannan and Han, Bo and Gong, Chen and Niu, Gang and Sugiyama, Masashi},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	pages={6838--6849},
	year={2019}
}

@inproceedings{li2020gradient,
	title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
	author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
	booktitle={International conference on artificial intelligence and statistics},
	pages={4313--4324},
	year={2020},
	organization={PMLR}
}

@inproceedings{NeverovaNV19,
	author    = {Natalia Neverova and
	David Novotn{\'{y}} and
	Andrea Vedaldi},
	editor    = {Hanna M. Wallach and
	Hugo Larochelle and
	Alina Beygelzimer and
	Florence d'Alch{\'{e}}{-}Buc and
	Emily B. Fox and
	Roman Garnett},
	title     = {Correlated Uncertainty for Learning Dense Correspondences from Noisy
	Labels},
	booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
	on Neural Information Processing Systems 2019, NeurIPS 2019, December
	8-14, 2019, Vancouver, BC, Canada},
	pages     = {918--926},
	year      = {2019},

}
@inproceedings{goel2021robustness,
	title={On the Robustness of Monte Carlo Dropout Trained with Noisy Labels},
	author={Goel, Purvi and Chen, Li},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={2219--2228},
	year={2021}
}

@article{northcutt2021confident,
	title={Confident learning: Estimating uncertainty in dataset labels},
	author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
	journal={Journal of Artificial Intelligence Research},
	volume={70},
	pages={1373--1411},
	year={2021}
}

@article{lenz2015deep,
  title={Deep learning for detecting robotic grasps},
  author={Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
  journal={The International Journal of Robotics Research},
  volume={34},
  number={4-5},
  pages={705--724},
  year={2015},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{bekiroglu2019benchmarking,
  title={Benchmarking protocol for grasp planning algorithms},
  author={Bekiroglu, Yasemin and Marturi, Naresh and Roa, M{\'a}ximo A and Adjigble, Komlan Jean Maxime and Pardi, Tommaso and Grimm, Cindy and Balasubramanian, Ravi and Hang, Kaiyu and Stolkin, Rustam},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={2},
  pages={315--322},
  year={2019},
  publisher={IEEE}
}


@article{nadarajah2005generalized,
  title={A generalized normal distribution},
  author={Nadarajah, Saralees},
  journal={Journal of Applied statistics},
  volume={32},
  number={7},
  pages={685--694},
  year={2005},
  publisher={Taylor \& Francis}
}

@Comment{jabref-meta: databaseType:bibtex;}
